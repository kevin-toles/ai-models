# =============================================================================
# AI Models Registry
# =============================================================================
# This file defines all available models and their metadata.
# Actual model files are stored in ../models/ and are gitignored.
# =============================================================================

version: "1.0.0"

models:
  phi-4:
    name: "Microsoft Phi-4"
    description: "General reasoning, summarization, coding"
    file: "phi-4/phi-4-Q4_K_S.gguf"
    size_gb: 8.4
    context_length: 4096  # Reduced - full 16K needs more RAM
    quantization: "Q4_K_S"
    source:
      repo: "microsoft/phi-4-gguf"
      filename: "phi-4-Q4_K_S.gguf"
    roles:
      - primary
      - thinker
      - coder
    # GPU layers: -1=all GPU (Metal), 0=CPU only, N=hybrid (N layers on GPU)
    # phi-4 has 40 layers. Using CPU-only for stability on 16GB Mac
    gpu_layers: 0  # CPU-only mode - slower (~57s) but stable, avoids memory crashes

  deepseek-r1-7b:
    name: "DeepSeek R1 Distill 7B"
    description: "Chain-of-thought reasoning, complex analysis"
    file: "deepseek-coder-v2-lite/deepseek-coder-v2-lite-instruct-q4_k_m.gguf"
    size_gb: 4.7
    context_length: 4608  # Reduced from 32768 - matches Qwen models for 16GB Mac stability
    quantization: "Q4_K_M"
    source:
      repo: "unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF"
      filename: "DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf"
    roles:
      - thinker
    gpu_layers: -1  # All layers on GPU (Metal)

  qwen2.5-7b:
    name: "Qwen 2.5 7B Instruct"
    description: "Code generation, technical tasks"
    file: "qwen2.5-7b/qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf"
    size_gb: 4.5
    context_length: 32768
    quantization: "Q4_K_M"
    source:
      repo: "Qwen/Qwen2.5-7B-Instruct-GGUF"
      filename: "qwen2.5-7b-instruct-q4_k_m.gguf"
    roles:
      - coder
      - primary
      - fast
    gpu_layers: -1  # All layers on GPU (Metal)

  llama-3.2-3b:
    name: "Llama 3.2 3B Instruct"
    description: "Fast responses, simple queries"
    file: "llama-3.2-3b/llama-3.2-3b-instruct-q4_k_m.gguf"
    size_gb: 2.0
    context_length: 8192
    quantization: "Q4_K_M"
    source:
      repo: "hugging-quants/Llama-3.2-3B-Instruct-Q4_K_M-GGUF"
      filename: "llama-3.2-3b-instruct-q4_k_m.gguf"
    roles:
      - fast
    gpu_layers: -1  # All layers on GPU (Metal)

  phi-3-medium-128k:
    name: "Phi-3 Medium 128K Instruct"
    description: "Long context processing (128K), document analysis"
    file: "phi-3-medium-128k/Phi-3-medium-128k-instruct-Q4_K_M.gguf"
    size_gb: 8.6
    context_length: 8192  # Reduced from 131072 - 128K context needs 32GB+ RAM
    quantization: "Q4_K_M"
    source:
      repo: "bartowski/Phi-3-medium-128k-instruct-GGUF"
      filename: "Phi-3-medium-128k-instruct-Q4_K_M.gguf"
    roles:
      - longctx
      - thinker
    gpu_layers: 0  # CPU-only - 8.6GB model + context exceeds 16GB Mac GPU
    notes: "Full 128K context requires 32GB+ RAM - reduced to 8K for 16GB Mac"

  gpt-oss-20b:
    name: "GPT-OSS 20B"
    description: "High-capacity model for complex reasoning (server only)"
    file: "gpt-oss-20b/gpt-oss-20b-Q4_K_M.gguf"
    size_gb: 11.6
    context_length: 32768
    quantization: "Q4_K_M"
    source:
      repo: "unsloth/gpt-oss-20b-GGUF"
      filename: "gpt-oss-20b-Q4_K_M.gguf"
    roles:
      - primary
      - thinker
    gpu_layers: -1  # All layers on GPU (requires 24GB+ VRAM)
    notes: "Requires >24GB RAM/VRAM - server deployment only"

  granite-8b-code-128k:
    name: "IBM Granite 8B Code Instruct 128K"
    description: "Code-focused model with 128K context for full-file analysis"
    file: "granite-8b-code-128k/granite-8b-code-instruct-128k.Q4_K_M.gguf"
    size_gb: 4.5
    context_length: 8192  # Reduced from 131072 - 128K context needs 32GB+ RAM
    quantization: "Q4_K_M"
    source:
      repo: "mradermacher/granite-8b-code-instruct-128k-GGUF"
      filename: "granite-8b-code-instruct-128k.Q4_K_M.gguf"
    roles:
      - coder
      - longctx
    gpu_layers: -1  # Fits in GPU at reduced context
    notes: "Full 128K context requires 32GB+ RAM - reduced to 8K for 16GB Mac"

  granite-20b-code:
    name: "IBM Granite 20B Code Instruct"
    description: "High-capacity code model (server only)"
    file: "granite-20b-code/granite-20b-code-instruct.Q4_K_M.gguf"
    size_gb: 12.8
    context_length: 8192
    quantization: "Q4_K_M"
    source:
      repo: "mradermacher/granite-20b-code-instruct-GGUF"
      filename: "granite-20b-code-instruct.Q4_K_M.gguf"
    roles:
      - coder
      - thinker
    gpu_layers: -1  # All layers on GPU (requires 24GB+ VRAM)
    notes: "Requires >24GB RAM/VRAM - server deployment only"

  # ---------------------------------------------------------------------------
  # Qwen3 Series (Added 2025-12-31)
  # ---------------------------------------------------------------------------
  qwen3-8b:
    name: "Qwen3 8B"
    description: "General-purpose Qwen3 model, good at code - pairs well with DeepSeek for critique mode"
    file: "qwen3-8b/Qwen3-8B-Q4_K_M.gguf"
    size_gb: 4.7
    context_length: 2048  # Reduced for stability on 16GB Mac
    quantization: "Q4_K_M"
    source:
      repo: "Qwen/Qwen3-8B-GGUF"
      filename: "Qwen3-8B-Q4_K_M.gguf"
    roles:
      - coder
      - primary
    gpu_layers: -1  # All layers on GPU (Metal)
    notes: "Successor to qwen2.5-7b - use in D4v2 preset with deepseek-r1-7b"

  qwen3-coder-30b-a3b:
    name: "Qwen3 Coder 30B-A3B MoE"
    description: "Mixture of Experts code model - 30.5B params, 3.3B active per token"
    file: "qwen3-coder-30b-a3b/Qwen3-Coder-30B-A3B-Instruct-Q3_K_M.gguf"
    size_gb: 14.0
    context_length: 32768  # Native 256K, reduced for safety
    quantization: "Q3_K_M"
    source:
      repo: "unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF"
      filename: "Qwen3-Coder-30B-A3B-Instruct-Q3_K_M.gguf"
    roles:
      - coder
      - primary
      - thinker
    gpu_layers: -1  # MoE - only 3.3B params active per token
    notes: "MoE architecture: 128 experts, 8 active per token. Standalone code generation."
    architecture:
      type: "moe"
      total_params: "30.5B"
      active_params: "3.3B"
      experts: 128
      experts_per_token: 8

  # ---------------------------------------------------------------------------
  # Vision-Language Models (VLM)
  # ---------------------------------------------------------------------------
  deepseek-vl2-tiny:
    name: "DeepSeek VL2 Tiny"
    description: "Vision-Language model for image understanding and classification"
    type: "vision"  # New type for VLM models - requires special provider
    file: "deepseek-vl2-tiny/"  # Directory containing safetensors
    size_gb: 3.2
    context_length: 4096
    source:
      repo: "deepseek-ai/deepseek-vl2-tiny"
      type: "transformers"  # Not GGUF - uses HuggingFace transformers
    roles:
      - vision
    gpu_layers: -1  # All on Metal
    notes: "1B activated params (MoE), for technical diagram classification. Use with Layer 2b refinement."
    architecture:
      type: "moe"
      total_params: "3.4B"
      active_params: "1.0B"
      vision_encoder: true

# =============================================================================
# Role Definitions
# =============================================================================
roles:
  primary:
    description: "Default/general purpose model"
    task_types:
      - general
      - summarize
      - explain
      - chat

  fast:
    description: "Quick responses, simple tasks"
    task_types:
      - simple
      - quick
      - classify
      - extract

  coder:
    description: "Code generation and review"
    task_types:
      - code
      - debug
      - review
      - refactor
      - test

  thinker:
    description: "Complex reasoning and analysis"
    task_types:
      - analyze
      - reason
      - compare
      - debate
      - plan

  longctx:
    description: "Long document processing"
    task_types:
      - document
      - summarize_long
      - rag

  vision:
    description: "Image understanding and visual reasoning"
    task_types:
      - image_classify
      - image_describe
      - diagram_analysis
      - ocr
